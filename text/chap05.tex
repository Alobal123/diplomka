\chapter{Experiments and Results}
\label{sec:chap5}
In this chapter we describe the setup and results of our experiments. We introduce used hardware, methods of training and discuss our results.

\section{Hardware}
We conducted all our experiments on the same machine running Linux operating system. It was equipped with two AMD RYZEN Threadripper 1950X CPU units (16 cores each) and 126 GB of RAM. However, we did not use this directly as it is much more efficient to train neural networks on GPUs. We had four NVIDIA GeForce GTX1080 Ti GPUs at our disposal, however we conducted our experiments only using one of them. We opted for this as we wanted to have some fair comparison of training time and not all the frameworks support running on multiple GPUs, or at least not without some heavy modifications of the code. 

\section{Accuracy}
Accuracy is a simple metrics computed as the number of correctly classified examples divided by the number of all examples. It is usually given in percents and is a standard for classification task evaluation. As is the case with our datasets, when the examples are not distributed equally, accuracy can be skewed by more numerous categories. Therefore we additionally compute \textit{average class accuracy}, which is computed as $\frac{1}{N}\sum_{i=1}^{N} {\frac{\;correctly\;classified\;in category\;i}{total\;in\;category\;i}} $, where $N$ is the number of categories. We believe that this number is somewhat more descriptive. 

\section{Testing on Artificial Dataset}
To compare the performance of various methods, it is necessary to use some standardized datasets. We chose ModelNet40 and ShapeNetCore for this purpose, their description is to be found in \autoref{sec:dataset}. \par
As the main goal of this thesis is to explore the possibilities of using the neural networks in practice, we are not interested in chasing couple of percents on artificial datasets. Rather we focus on general performance and convenience of use. Thus we did not spend huge amounts of time on hyperparameter tuning in order to increase the accuracy as this would lead to overtraining of hyperparameters on the test set. Although this problem could be solved by employing a validation set, standard ModelNet40 split does not support this option and hyperparameter tuning would be cumbersome to do in a real-world setting. So we opted for using the hyperparameters described in the original papers if available or used the default setting in the original code. More information about the hyperparameter setting can be found in \autoref{Attachment:params}. \par
Another important decision was to choose the stopping condition of training. As each of the networks takes different time to complete one training epoch (a period during which each training example is presented once) it would be not fair to set some fixed number of epochs. Therefore we stop the training after convergence, i.e., when the value of the loss function on the test set is not improving for several epochs. Some of the networks do not really stabilize in test set, so we wait for the stabilization of the training loss and report the accuracy of the test set averaged over the last ten epochs. More detailed statistics can be found in \autoref{Table:details} and \autoref{Table:detailss} for ModelNet40 and ShapenetCore respectively.
\input{./tables/accuracies.tex}

\subsection{Time and Memory Requirements}
\autoref{Table:time} shows approximate times of training. These times are not conclusive, as training time depends on the used hardware. However, the differences among the types of networks are considerable. In general we can say that the point cloud based networks are quite fast, processing tens of examples each second. Octree and Adaptive-octree networks proved to be very the most efficient - processing hundred and two hundred examples per second respectively. Although multi-view based networks achieve better accuracies, they are considerably slower. Depending on the network, the simplest architecture can process tens of examples per second, deeper networks require more time, processing around ten examples per second. The Seq2Seq network seems very fast and it indeed is, but the fact that the separate fine-tuning of VGG is required to use this network, must be considered. The slowest by far is voxel-based VRN which needs two seconds for a a single training example and requires approximately a week to train. \par
\input{./tables/time.tex}
\input{./tables/memory.tex}
As for the memory requirements, \autoref{Table:memory} shows approximate sizes of the neural networks, which roughly correspond to the number of trainable parameters of the networks and also memory requirements during inference. Memory usage during training depends on the batch size of the network and size of the input. In our experiments we tried to increase the batch size so the GPU memory was maximally utilized. 


\subsection{Results on ModelNet40}
We trained and tested all the networks introduced previously and all the input data variants on ModelNet40 dataset (described in \autoref{sec:modelnet}). The results we achieved with comparison to reported accuracies are listed in \autoref{Table:accs}. 

In this section we go into detail about results of individual networks. In this section by “parameters” we mean non-trainable hyperparameters such as learning rates, momentum value, number of training epochs, etc.

\def\myitem #1#2{
	\item { \textbf{#1} \par #2
	}
}

\begin{itemize}
	\myitem{VRN (Voxception Residual Network)}{	Authors report an accuracy of 91.33\% and we were able to achieve 90.32\%. The original paper does not provide exact training parameters, so we used the default parameters of the code. This might be caused by using our own data, created by our own script which can be inferior to original voxelization. Authors provide some example data but not complete ModelNet40. We did not perform additional experiments because of the long training time of this network.}
	\myitem {O-CNN and AO-CNN (Octree and Adaptive Octree convolutional neural networks)}{Authors report accuracy of 90.6\% and 90.5\% for O-CNN and AO-CNN respectively. We were able to achieve o 88.29\% and 91.08\%. To prepare the data, we used scripts provided by authors and they also state the exact training parameters. } 
	\myitem{Multi-view convolutional neural networks}{According to \cite{su_deeper_2018} the quality of the input images is not negligible in the case of multi-view based approaches to 3D classification. We have tested these networks on four different sets of images. First, the original images provided by \cite{su_multi-view_2015}, then our own images rendered in pbrt and two variants of images rendered by scripts provided by \cite{su_deeper_2018}.  Detailed results can be found in \autoref{Table:mv}. The technique of rendering the images is discussed in \autoref{subsec:meshtoimgs}.}
	\input{./tables/multiview.tex}
	
	\myitem{MVCNN (Multi-view Convolutional Neural Network)}{Authors report accuracy of 90.1\% on images rendered using phong shading, but we achieved accuracy of only 83.99\%. We were able to get higher accuracy of 88.83\% on newer shaded images. However the authors do not provide the training parameters in the paper so we had to default to parameters used in the code.}
	
	\myitem{MVCNN2 (Multi-view Convolutional Neural Network 2)}{Authors report the highest achieved accuracy of 95\% on the shaded variant of the images. We achieved only 90.64\% on the shaded images and 90.52\% on the depth images. The training parameters are not provided in the paper, so we used the default values from the code.}
	
	\myitem{RotationNet}{Authors report accuracy of 97.37\% using original phong shaded images by \cite{su_multi-view_2015} but we have been able to achieve only 92.12\%. The  reported value of 97.37\% is however a maximum achieved over more training sessions. Authors report average accuracy of 93.70\% using AlexNet as the pretrained image network, which is much closer to our result. They do not give all the training parameters in the paper but most of the important ones are mentioned.}
	
	\myitem{SEQ2SEQ (Sequential Views to Sequential Labels)}{Authors claim that they achieved accuracy of 92.5\% only with VGG and voting across twelve views. We failed to replicate this and and achieved only 90.86\%. Therefore we could not achieve the reported 93.31\% accuracy of the Seq2Seq network but only measured 91.26\%. However this means that the single view VGG with voting performed better than most of the more complex networks and the recurrent Seq2Seq did not bring any significant improvement.}
	
	\myitem{PointNet and PointNet++}{The authors report accuracy of 89.2\% and 91.9\% for PointNet and PointNet++ respectively. We were able to achieve only 86.60\% and 89.00\%. Authors provide most of the training parameters in their papers, so we use them along with the default values given in the code. We trained with point cloud data provided by authors as well as our own converted data (sampling techniques are described in \autoref{subsec:pointcloud})}. The full results of these tests are given in \autoref{Table:pn}. 
	
	\input{./tables/pointnets.tex}
	
	\myitem{SO-NET (Self-Organizing Network for Point Cloud Analysis)}{The authors report the highest achieved accuracy of 93.4\% on an experiment with 5000 input points. They also provide most of the training parameters in their paper. We were able to achieve only 88.90\%. We used our own sampling of point cloud which can be the source of the problem.}
	\myitem{KD Network}{The authors report the highest achieved accuracy of 91.8\% and we have achieved only 88.10\%. Authors do not provide all the training parameters so we defaulted to the values used in code. All the code to sample point clouds and construct the trees we used is supplied by the authors so this could not be the source of the disparity between reported and measured accuracy.}
\end{itemize}

\subsection{Difficult Categories}
In this section we explore the results in more detail -- we discuss the accuracies on individual categories of ModelNet40. We show which categories are generally hard to recognize and which, on the other hand, did not cause any problems. We also give a brief account about pairs of categories which were mistaken most often and their illustrations. We believe that this information can be useful when designing custom category hierarchies. \par
We compute the accuracies per class and average these across all the trained networks. There are several categories which were almost always correctly classified and none of the networks failed to learn it fairly quickly. These categories are airplane, laptop, guitar and keyboard, which all achieved more than 99.00\% average accuracy, 100\% for most of the networks. Another successfully recognized categories are "car", "bed", "chair", "monitor", "person", "bottle" and "sofa", all achieving more than 95\% average accuracy.  
On the other hand the most difficult category by far was a “flower pot”, which was recognized only in 14.6 percent of cases. This is probably caused by the small number of examples of this category as well as very similar categories of “flower” and “vase”. Other generally difficult categories include "wardrobe", "cup", "night stand", "bench" and "radio", achieving no more than 75\% accuracy. You can find average accuracies per category in \autoref{Table:cataccuracies} \par

\input{./tables/cataccuracies.tex}

When we take a look at the pairs of categories most often mistaken one for the other, we find out that besides the above mentioned “flower pot”, the most mistakes were made classifying a “table” as a “desk”. This sounds quite reasonable, as the borderline between these categories is blurry even by human standards. The same applies for the category of “wardrobe”, which was commonly mistaken for “bookshelf”, “dresser” or even an “xbox”. The most commonly made mistakes can be found in \autoref{Table:mmistakes}.

\input{./tables/mmistakes.tex}


\section{ShapeNetCore Results}
We conducted several experiments on ShapeNetCore dataset (\autoref{sec:shapenetcore}). It is about five times bigger than ModelNet40 and has 55 categories. The training of the networks therefore takes five times longer and so we did not train on all variants of inputs as with ModelNet40, but chose only those which were most successful previously. \autoref{Table:saccs} shows the achieved accuracies on ShapeNetCore dataset.
As can be seen from the table, the differences in measured accuracies across the networks are greater than in the case of ModelNet40, so the comparisons are more clear. The Multi-view networks, achieving around 92\%, are performing much better than the point cloud based networks. Voxel based VRN achieves reasonable accuracy of 88.98\%, but one training epoch takes about 30 hours. It is possible that this result can be improved by training for longer time as we managed to train the network only for ten epochs. The octree based networks are reasonably fast and achieve accuracy comparable to multi-view networks.  \par
Also the differences between the accuracy and the average class accuracy is much higher. This is probably caused by the fact, that the categories in ShapeNetCore dataset are not populated equally at all.\par

\input{./tables/accuraciesshapenet.tex}

The detailed per category accuracies are to be found in \autoref{Table:cataccs}. The category of "telephone" has a low accuracy of 28\% which was caused by bad division of this category and the category "cellular phone" in the original ShapeNet category division.
\input{./tables/cataccs.tex}


