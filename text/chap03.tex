\chapter{Survey of 3D Classification Methods}
\label{sec:chap3}
In recent years many techniques for classifying 3D shapes by means of artificial neural networks have been devised. In this chapter we present most of the commonly used and successful of them. As mentioned in previous chapters the usual mesh format is not suitable for processing by a neural network directly so we divide the networks according to the format they use as their input: voxel-based, multi-view, and point-cloud-based. \autoref{Table:1} shows a list of neural networks described in this chapter.

\input{./tables/survey.tex}

\section{Voxel-based Neural Networks}
As 2D convolutional neural networks were a great breakthrough in image recognition, it is natural to try to generalize this approach to three dimensions. Instead of pixels we use 3D occupancy grid of so called voxels. Also convolutions can be easily extended to work in three dimensions. Convolutions seem to be suitable for the task as they can make use of the spatial structure of the problem. However in 3D, they are computationally demanding and voxel grids have high memory requirements as their size grows with the cube of the resolution. For this reason only relatively small resolutions can be used, the most usual being $32^3$. 

\subsection{VoxNet}
First of the successful systems applying 3D convolutions to occupancy grids is VoxNet (\cite{maturana_voxnet:_2015}), which we will use as an example of a network using a shallow convolutional architecture. 
In VoxNet, the occupancy grid is processed by 3D convolutions which extract local features and lower the resolution. The convolution result is passed to a ReLU layer to achieve nonlinearity. Maximum pooling is then performed in order to get better representation and to further lower the number of parameters needed. Finally the occupancy grid is flattened and passed to a fully connected layer which outputs resulting feature vector. \autoref{voxnet} shows a diagram of the VoxNet architecture.\par  
As is common with the neural networks, data augmentation is a very important part of the training process. VoxNet uses rotation along the vertical axis as its main augmentation technique. During training it creates n copies of each input instance each rotated by $360/n$ degrees. Typical values of $n$ range from 8 to 24. At evaluation time it presents all rotations of the input object to the network and then uses pooling across the rotations to get the class prediction. There is a TensorFlow implementation of VoxNet (\cite{maturana_voxnet_2016}) available.\par
Results of VoxNet were improved by Orientation boosted voxel nets for 3D Object Recognition (\cite{sedaghat_orientation-boosted_2016}), wherein the classification task was augmented
with an orientation estimation task. There is an implementation in Caffe available (\cite{sedaghat_orion_2016}). FusionNet (\cite{hegde_fusionnet:_2016}) combines 3D convolutions on voxel representation with a multi-view approach.

\input{./img/voxnet.tex}

\subsection{Voxception Residual Network}
Voxception Residual Network (\cite{brock_generative_2016}) is inspired by deep residual convolutional networks for image recognition which are the state of the art approach for this task. It uses Inception-style modules (\cite{szegedy_inception-v4_2016}), batch normalization (\autoref{sec:regularization}), residual connections (\autoref{sec:training}) and stochastic network depth (\cite{huang_deep_2016}). The Voxception network consists of several sequential voxception modules. These modules  should enable for information to propagate through the network through many possible “pathways”, while still maintaining simplicity and efficiency. \par
For example, one of the basic blocks concatenates the result of a $3\times3\times3$ convolution and the result of a $1\times1\times1$ convolution, so the network can learn which of these filters to apply. A diagram of this block with added residual connection is in \autoref{voxblock}. There are several types of the so called Voxception blocks with residual connections with pre-activation\footnote{Nonlinearity is used before the addition.} employed in the network. \par
The best performing architecture consists of Voxception blocks as well as downsampling blocks which enable the network to choose the best downsampling methods (e.g. convolutions with stride bigger than one or pooling). The deepest path through the network is 45 layers, and the shallowest path (assuming all droppable non-residual paths are dropped) is 8 layers deep. \autoref{vrn} shows a diagram illustrating the Voxception Residual Network architecture.
The model is quite big and slow to train, authors report one epoch taking around six hours on a single Titan X GPU for ten thousand training examples, which is in line with our results. 
Voxel grid resolution of $32^3$ is used. The network is trained using 24 rotations of each input instance along the vertical axis and using binary voxel representation but with binary voxel range $\{-1,5\}$ instead $\{0,1\}$  to encourage the network to pay more attention to positive entries. \par
As there is not much new research done in the area of voxel based classification we chose a single Voxel Residual Network as a representative of this category. It still achieves accuracy comparable to the latest networks and has publicly available code. It is implemented in Theano with Lasagne (\cite{brock_vrn_2016}) and offers several models to train and make ensemble from. We opted for only one of these models as it is very time demanding to train even a single network. The model chosen is the model reported by authors as the best one and described in detail in the original paper. 
This approach still represents the state of the art in voxel based classification as ensemble of similar models reports 95.54\% accuracy on ModelNet40 dataset which remains one of the highest reported.

\input{./img/voxblock.tex}
\input{./img/vrn.tex}

\subsection{Octree and Adaptive Octree Networks}
Octree-based Convolutional Neural Network (\cite{wang_o-cnn:_2017}) uses another data structure for representing 3D data -- an octree (\cite{meagher_octree_1980}). Octree is a tree where each node has exactly eight children, partitioning the space into finer and finer cubes. This basically means voxelization of the 3D model, but in this case only voxels on the borders are considered. This can be implemented efficiently and represented in a format suitable for GPU computation. In each leaf node a normal vector of the surface is stored. Authors then present an efficient way of performing convolutions on octrees and construct a hierarchical structure of shared layers for individual levels of the tree. The computation proceeds from the finest leaf octants and continues upwards to the root of the tree.
This approach gives good results but the octrees are of a fixed maximum depth and therefore can waste memory on flat regions where a simple planar approximation would be sufficient. This problem is solved by Adaptive Octree (\cite{wang_adaptive_2018}) representation which uses such planar patches as a representation in leaf nodes. Therefore flat areas of the original mesh can be represented by a simple leaf on a higher level of a tree, while more complex areas are subdivided into finer details. 
Several (12 in our case) rotations of the 3D model are used during both training and evaluation to achieve better results.
Authors offer an implementation of both networks in Caffe as well as tools for converting mesh data to octrees and adaptive octrees (\cite{wang_o-cnn_2018}).

\section{Multi-view-based Neural Networks}
Another approach, harnessing the power of 2D convolutions and huge image datasets, are the so called multi-view neural networks.
A general setup of multi-view networks is as follows: they use rendered images of the 3D model from different angles as an input, these views are then passed to some pre-trained image processing network and then some technique of combining features from different views is employed. Such techniques range from simple pooling across the views to employing recurrent neural network to process them as a sequence.\par
Multi-view approaches can be considered state of the art in this area as they achieve excellent results. For a fair comparison of these methods we use the same sets of images and twelve views of each 3D model rotated along the vertical axis.

\subsection{Multi-view Convolutional Networks}
For training a multi-view based network we need to fine-tune some already pre-trained image recognition network. We use two different networks: smaller and older AlexNet (\cite{krizhevsky_imagenet_2012}) and the state of the art deep network VGG (\cite{simonyan_very_2014}). This offers a simple method of 3D classification; we train the image network on the views of a rotated 3D model without any regard for the multi-view nature of the dataset. During evaluation we perform voting across the views. We chose VGG for this task as it performs better on image recognition tasks. We use a publicly available implementation of VGG (\cite{machrisaa_vgg_2017}) with 19 weighted layers in Tensorflow. As we shall see later, even this simple approach yields results comparable with the most sophisticated networks.
\par
The first multi-view approach to appear (\cite{su_multi-view_2015}) uses shared convolutional layers individual views, then uses max pooling across the views to combine the features. Resulting features are fed to another convolutional network and then classified. We chose to test this approach as it is the first multi-view approach to achieve good results and it is simple enough to serve as a baseline for similar approaches. From several available implementations of this network we have chosen a Tensorflow implementation (\cite{lee_multi-view_2016}). In this case, we use AlexNet as the pre-trained image network, which is recommended by the authors of the code and supported in the code structure. \autoref{mvcnn} shows a diagram illustrating the multi-view architecture. \par
The revisited but similar approach is used by \cite{su_deeper_2018}, which divides the training phase into two stages. In the first stage the network is trained only using one view at a time and later during the second phase pooling across the views is employed. Authors also explore different pre-trained image network architectures and different image rendering techniques improving accuracy of this method significantly. It uses a pre-trained VGG convolutional network as its base. It offers a publicly available implementation in PyTorch (\cite{su_multi-view_2018}) which we have chosen to test as it promises some of the best accuracies achieved on ModelNet40 so far.

\input{./img/mvcnn.tex}

\subsection{RotationNet}
RotationNet (\cite{kanezaki_rotationnet:_2018}) reports the highest achieved accuracy on the ModelNet40 dataset so it is of particular interest to us. It combines the multi-view classification with an unsupervised pose estimation task. Unlike other multi-view networks, it does not provide information about the position of the viewpoints to the network, i.e. they can be rotated arbitrarily, hence the name of the network. 
To achieve this, a new category is added to the set of original classification categories. The meaning of this category is “this is not the correct viewpoint”. All the possible rotations of the viewpoints are tried and the most probable according to predicted categories is chosen. Authors offer two implementations, one in Caffe (\cite{kanezaki_rotationnet_2018}), and another one in PyTorch. We have chosen to test this network as it reports very high accuracy on ModelNet40 and inherently contains pose estimation, which can be useful for future work.

\subsection{Sequential Views to Sequential Labels}
Sequential Views to Sequential Labels network (\cite{zhizhong_seqviews2seqlabels:_2018}) employs recurrent neural networks and treats multiple views as a sequence of images. It uses classic encoder-decoder architecture. In order to do this it treats its output not as a single vector but as a sequence of labels. It uses pre-trained convolutional network, which is fine-tuned on single-image classification task. We opted for the VGG network already described above. The last fully connected layer of size 4096 is used as a feature vector for single views and fed into the encoder as a sequence. Both the encoder and decoder consists of GRU (\autoref{sec:rnns}) cells and attention is used. An implementation in TensorFlow is available (\cite{zhizhong_seq2seq_2018}) and we used it to test this network.

\section{Point-cloud-based Neural Networks}
An altogether different representation of spatial data is a point cloud. A point cloud is an unordered set of points in Euclidean space, representing the surface of the object. It is a natural output format of laser scanning devices used by robots or autonomous cars and also in medical scanning. We can easily construct a point cloud from a mesh by sampling its faces. Point clouds are neither structured nor ordered as voxels or images are, which poses a problem to neural networks.

\subsection{PointNet and PointNet++}
The first network to successfully overcome all the difficulties of processing unordered point clouds was PointNet (\cite{qi_pointnet:_2016}). Its main idea lies in using only symmetric functions i.e., functions for which the order of arguments does not matter. Each point is processed independently by a series of multi-layer perceptrons sharing weights. Then a global feature vector is constructed using maximum pooling, which is a symmetric function. Another important feature of PointNet are learn-able geometric transformations which ensure some invariance to rotation or jittering (random small translations) of input point cloud. Rotation and jittering are also used as data augmentation during the training. \autoref{pointnet} shows a diagram of the PointNet architecture.
Although PointNet achieves reasonable results it does not provide any mechanism for learning local features.\par 
PointNet++ (\cite{qi_pointnet++:_2017}) presents a hierarchical structure inspired by convolutional neural networks which solves the problem of extracting local features. It clusters close sets of points together and runs original PointNet on such neighborhoods. For this purpose iterative farthest point sampling and multi-resolution grouping (which ensures good representation for differently dense areas) are used. Thusly obtained local features are represented by the centroid of the original neighborhood and clustered again in a hierarchical manner. Finally a classic fully connected layer is employed for extracting global features and classification.
Several (12 in our case) rotations of the 3D model are used during both training and evaluation to achieve better results. There is an implementation of both networks available in TensorFlow (\cite{qi_pointnet_2016, qi_pointnet++_2017}) and we tested both of them as they achieve reasonable accuracy and promise better scalability and are considerably faster than some other methods.

\input{./img/pointnet.tex}

\subsection{Self-Organizing Network}
The PointNet++ architecture lacks the ability to reveal the spatial distribution of the input point cloud during the hierarchical feature extraction. Self-Organizing Network (\cite{li_so-net:_2018}) solves this problem by constructing \textit{self organizing map} (SOM) which represents the point cloud better than simple centroids used in PointNet++. Each point of the original point cloud is associated with $k$ nearest SOM nodes and for each such node a mini point cloud is constructed. This also ensures that the mini point clouds are overlapping which was shown to be a key feature. These mini point clouds are processed by a series of fully connected layers similar to the original PointNet. This process yields a local feature vector for each of the original SOM nodes, which are then used for constructing a global feature vector by means of max pooling across the nodes. There is an implementation in PyTorch (\cite{li_sonet_2018}) available, which contains also code for creating self organizing maps from point clouds. We have chosen to test this network as it seems to offer significant improvement for a cost of only quick data preprocessing.

\subsection{KD-Network}
A kd-tree (\cite{bentley_multidimensional_1975}) is a data structure suitable for storing and searching in a set of points of higher dimension. Its 3D variant is used as an input format for KD-Network (\cite{klokov_escape_2017}). Firstly kd-tree is constructed over a point cloud. Then the tree is fed to a series of fully connected layers in a recursive manner starting in the leaf nodes and continuing to the root, where the global feature vector is extracted and used for classification. Weights of the fully connected layers are shared for nodes on the same level at the tree, where the tree is split along the same coordinate. \par
During training it uses several geometric perturbations as data augmentation as well as randomized kd-tree construction. This approach can process raw point clouds but requires heavy preprocessing when constructing the kd-trees.
An implementation in Theano with Lasagne (\cite{klokov_kd-net_2017}) is supplied by the authors, which also provides a framework for kd-tree construction from a point cloud.

\subsection{Graph Based Convolutional Network}
For the completeness’ sake we mention a graph based approach by \cite{dominguez_general-purpose_2018}, which constructs a graph from a point cloud. Vertices of the graph are the original points and edges are constructed to the six nearest neighbors and sorted by their direction by an arbitrary sorting. Then special graph convolutions are applied repeatedly simplifying the structure of the graph and extracting local features. This approach is interesting from the theoretical standpoint but the training is very slow and it does not achieve state of the art results. There is a TensorFlow implementation of this network available (\cite{dominguez_g3dnet_2018}).



