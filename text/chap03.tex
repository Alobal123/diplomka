\chapter{Survey of 3D Classification Methods}
\label{sec:chap3}
In recent years many techniques for classifying 3D shapes by means of artificial neural networks have been devised. In this chapter we present most of the commonly used and successful of them. As mentioned in previous chapters the usual mesh format is not suitable for processing by a neural network directly so we divide the networks according to the format they use as their input: voxel-based, multi-view, and point-cloud based. \autoref{Table:1} shows a list of neural networks described in this chapter.

\input{./tables/survey.tex}

\section{Voxel Based Neural Networks}
As 2D convolutional neural networks were a great breakthrough in image recognition, it is natural to try to generalize this approach to three dimensions. Instead of pixels we use 3D occupancy grid of so called voxels. As shown later we can easily extend convolutions to three dimensions. Convolutions seem to be suitable for the task as they can make use of the spatial structure of the problem. However they are computationally demanding and voxel grids have high memory requirements as their size grows with the cube of the resolution. For this reason only relatively small resolutions can be used, the most usual being $32^3$. 

\subsection{VoxNet}
First of the successful systems applying 3D convolutions to occupancy grids is VoxNet \cite{maturana_voxnet:_2015}, which we will use as an example of a network using a shallow convolutional architecture. 
In VoxNet, the occupancy grid is processed by 3D convolutions which extract local features and lower the resolution. The convolution result is then passed to a ReLU layer to achieve nonlinearity. Maximum pooling is then performed in order to get better representation and to further lower the number of parameters needed. Finally the occupancy grid is flattened and passed to a fully connected layer which outputs resulting feature vector. \autoref{voxnet} shows a diagram of the voxnet architecture.\par  
As is common with neural networks, data augmentation is a very important part of the training process. VoxNet uses rotation along the vertical axis as its main augmentation technique. During training it creates n copies of each input instance each rotated by 360/n degrees. Typical values of n range from 8 to 24. At evaluation time it presents all n rotations of the input object to the network and then uses pooling across the rotation to get the class prediction. \par
Results of VoxNet were improved by ORION \cite{sedaghat_orientation-boosted_2016}, wherein the classification task was augmented
with an orientation estimation task and FusionNet \cite{hegde_fusionnet:_2016} combining 3D convolutions on voxel representation with multi-view approach.

\input{./img/voxnet.tex}

\subsection{Voxception Residual Network}
Voxception Residual Network \cite{brock_generative_2016} is inspired by deep residual convolutional networks for image recognition which are the state of the art approach for this task. It uses Inception-style \cite{szegedy_inception-v4_2016} modules, batch normalization (\autoref[sec:regularization]), residual connections (\autoref{sec:training}) and stochastic network depth \cite{huang_deep_2016} The Voxception network consists of several sequential voxception modules. These modules  should enable for information to propagate through the network through many possible “pathways”, while still maintaining simplicity and efficiency. \par
For example, one of the basic blocks concatenates the result of a 3x3x3 convolution and the result of a 1x1x1 convolution, so the network can learn which of these filters to apply. Diagram of this block with added residual connection is in \autoref{voxblock}. There are several types of the so called Voxception blocks with residual connections with pre-activation (nonlinearity is used before the addition) employed in the network. \par
The best performing architecture consists of voxception blocks as well as downsampling blocks which enable the network to choose the best downsampling methods. The deepest path through the network is 45 layers, and the shallowest path (assuming all droppable non-residual paths are dropped) is 8 layers deep.
The model is quite big and slow to train, authors report one epoch taking around six hours on single Titan X GPU, which is in line with our results.
Voxel grid resolution of $32^3$ is used. The network is trained using 24 rotations of each input instance along the vertical axis and using binary voxel representation but with binary voxel range $\{-1,5\}$ instead $\{0,1\}$  to encourage the network to pay more attention to positive entries. \par
As there is not much new research done in the area of voxel based classification we chose a single Voxel Residual Network as a representative of this category. It still achieves accuracy comparable to the latest networks and has publicly available code. It is implemented in Theano with Lasagne and offers several models to train and make ensemble from. We opted for only one of these models as it is very time demanding to train even a single network. The model chosen is the model reported by authors as the best one and described in detail in the original paper. 
This approach still represents the state of the art in voxel based classification as ensemble of similar models reports 95.54\% accuracy on ModelNet40 dataset which remains one of the highest reported.

\input{./img/voxblock.tex}
\input{./img/vrn.tex}

\subsection{Octree and Adaptive Octree Networks}
Octree-based Convolutional Neural Network \cite{wang_o-cnn:_2017} uses another data structure for representing 3D data -- an octree \cite{meagher_octree_1980}. Octree is a tree where each node has exactly eight children so partitioning the space into finer and finer cubes. This basically means voxelization of the 3D model, but in this case only voxels on the borders are considered. This can be implemented efficiently and represented in a format suitable for GPU computation. In each leaf node a normal vector of the surface is stored. Authors then present an efficient way of performing convolutions on octrees and construct a hierarchical structure of shared layers for individual levels of the tree. The computation proceeds from the finest leaf octants and continues upwards to the root of the tree.
This approach gives good results but the octrees are of a fixed maximum depth and therefore can waste memory on flat regions where a simple planar approximation would be sufficient. This problem is solved by Adaptive Octree \cite{wang_adaptive_2018} representation which uses such planar patches as a representation in leaf nodes. Therefore flat areas of the original mesh can be represented by simple leaf on a higher level of a tree, while complicated areas are divided to finer details. 
Authors offer an implementation of both networks in Caffe as well as tools for converting mesh data to octrees and adaptive octrees.

\section{Multi-view Based Neural Networks}
Another approach, harnessing the power of 2D convolutions and huge image datasets, are the so called Multi-view neural networks.
A general setup of multi-view networks is as follows. They use rendered images of the 3D model from different angles as an input. These views are then passed to some pretrained image processing network and then some technique of combining features from different views is employed. Such techniques range from simple pooling across the views to employing recurrent neural network to process them as a sequence.\par
Multi-view approaches can be considered state of the art in this area as they achieve excellent results. For a fair comparison of these methods we use the same sets of images for training and testing and twelve views of each 3D model rotated along the vertical axis.

\subsection{Multi-view Convolutional Networks}
For training a multi-view based network we need to fine tune some already pretrained image recognition network. We use two different networks: smaller and older AlexNet \cite{krizhevsky_imagenet_2012} and the state of the art deep network VGG \cite{simonyan_very_2014}. This offers a simple method of 3D classification; we train the image network on a single rendered view of a rotated 3D model and then during evaluation we perform voting across all views. We chose VGG for this task as it performs better on image recognition tasks. We use a publicly available implementation of VGG \cite{machrisaa_tensorflow-vgg_nodate} with 19 weighted layers in Tensorflow. As we shall see later, even this simple approach yields results comparable with the most sophisticated networks.
\par
The first and simplest multi-view approach \cite{su_multi-view_2015} uses shared convolutional layers to process individual views, then uses max pooling across the views to combine the features. Resulting features are fed to another convolutional network and then classified. We chose to test this approach as it is the first multi-view approach to achieve good results and it is simple enough to serve as baseline for similar approaches. From several available implementations of this network we have chosen a Tensorflow implementation. As a pretrained image network we use AlexNet which is recommended by authors of the code and supported in the code structure. 
\par
The revisited but similar approach is used by \cite{su_deeper_2018}, which divides the training phase in two stages. In the first stage the network is trained only using one view and later during the second phase pooling across the views is employed. Authors also explore different pre-trained image network architectures and different image rendering techniques improving accuracy of this method significantly. It uses a pre-trained VGG-11 convolutional network as its base. It offers a publicly available implementation in Pytorch which we have chosen to test as its promises some of the best accuracies achieved on ModelNet40 so far.

\input{./img/mvcnn.tex}

\subsection{RotationNet}
RotationNet \cite{kanezaki_rotationnet:_2016} reports the highest achieved accuracy on ModelNet40 dataset so it is of particular interest to us. It combines the multi-view classification with an unsupervised pose estimation task. Unlike other multi-view networks we do not provide information about the position of the viewpoints, i.e. they can be rotated arbitrarily hence the name of the network. 
To achieve this, a new category is added to the set of original classification categories. The meaning of this category is “this is not the correct viewpoint”. All the possible rotations of the viewpoints are tried and the most probable according to predicted categories is chosen. Authors offer two implementations, one in Caffe the second one in PyTorch. We have chosen to test this network as it reports very high accuracy on ModelNet40 and inherently contains pose estimation, which can be useful for future work.

\subsection{Sequential Views to Sequential Labels}
Sequential Views to Sequential Labels network \cite{zhizhong_seqviews2seqlabels:_2018} employs recurrent neural networks and treats multiple views as a sequence of images. It uses classic encoder - decoder architecture. In order to do this it treats its output not as a single vector but as a sequence of labels. It uses pretrained convolutional network which is fine tuned on single images classification task. We opted for VGG-19 network already described above. The last fully connected layer of size 4096 is used as a feature vector for single views and fed into the encoder as a sequence. Both encoder and decoder consists of GRU cells and attention is used. Implementation in Tensorflow is available and we used it to test this network.

\section{Point Cloud Based Neural Networks}
An altogether different representation of spatial data is a point cloud. A point cloud is an unordered set of points in the Euclidean space. This is natural output format of laser scanning devices used by robots or autonomous cars and also in medical scanning. We can easily construct a point cloud from a mesh by sampling its faces. Point clouds are nor structured neither ordered as voxels or images are, which poses a problem to neural networks.

\subsection{PointNet and PointNet++}
The first network which successfully overcame all the difficulties of processing unordered point clouds was PointNet \cite{qi_pointnet:_2016} Its main idea lies in using only symmetric functions i.e. functions for which the order of arguments does not matter. So each point is processed independently by a series of multi-layer perceptrons sharing weights. Then a global feature vector is constructed using max pooling, which is a symmetric function. Another important feature of PointNet are learnable geometric transformations which ensure some invariability to rotation or jittering of input point cloud. Rotation and jittering are also used as data augmentation during training. Although PointNet achieves reasonable results it does not provide any mechanism for learning local features. \autoref{pointnet}
PointNet++ \cite{qi_pointnet++:_2017} presents a hierarchical structure inspired by convolutional neural networks which solves this very problem. It clusters close sets of points together and runs original PointNet on such neighborhoods. For this purpose iterative farthest point sampling and multi-resolution grouping (which ensures good representation for differently dense areas) are used. Thusly obtained local features are represented by the centroid of the original neighborhood and clustered again in a hierarchical manner. Finally a classic fully connected layer is employed for extracting global features and classification.
There is an implementation for both networks available in Tensorflow and we tested both of them as they achieve reasonable accuracy and promise better scalability and are considerably faster than other methods.

\input{./img/pointnet.tex}

\subsection{Self-Organizing Network}
The PointNet++ architecture lacks the ability to reveal the spatial distribution of the input point cloud during the hierarchical feature extraction. Self-Organizing Network \cite{li_so-net:_2018} solves this problem by constructing \textit{self organizing map} (SOM) which represents the point cloud better than simple centroids used in PointNet++. Each point of the original point cloud is associated with k nearest SOM nodes and for each such node a mini point cloud is constructed. This also ensures that the mini point clouds are overlapping which was shown to be a key feature. These mini point clouds are processed by a series of fully connected layers similar to the original PointNet. This process yields local feature vector for each of the original SOM nodes, which are then used for constructing a global feature vector by means of max pooling across the nodes. There is an implementation in Pytorch available which contains also code for training self organizing maps from point clouds. We have chosen to test this network as it seems to offer significant improvement for a cost of only quick data preprocessing.

\subsection{KD Network}
A kd-tree \cite{bentley_multidimensional_1975} is a data structure suitable for storing and searching in a set of points of higher dimension. Its 3D variant is used as an input format for KD-net \cite{klokov_escape_2017}. Firstly kd-tree is constructed over a point cloud. Then the tree is fed to a series of fully connected layers in a recursive manner starting in the leaf nodes and continuing to the root, where the global feature vector is extracted and used for classification. Weights of the fully connected layers are shared for nodes on the same level at the tree, where the tree is splitting along the same coordinate. \par
During training it uses several geometrical perturbations as data augmentation as well as randomized kd-tree construction. This approach can process raw point clouds but requires heavy preprocessing when constructing the kd-trees.
An implementation in Theano is supplied by the authors which also provides a framework for kd-tree construction from a point cloud.

\subsection{Graph Based Convolutional Network}
For the completeness’ sake we mention a graph based approach \cite{dominguez_general-purpose_2018}, which constructs a graph from a point cloud. Vertices of the graph are the original points and edges are constructed to the six nearest neighbors and sorted by their direction by some arbitrary sorting. Then special graph convolutions are applied repeatedly simplifying the structure of the graph and extracting local features. This approach is interesting from theoretical standpoint but the training is very slow and it does not achieve state of the art results.

\input{./tables/shapenet_cats.tex}
\input{./tables/modelnet_cats.tex}
