<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>conversions.html</title>
  </head>
  <body>
    <h1>Data conversion</h1>
    <p>This page describes the process of converting mesh files to specific
      formats in more detail. There are three main types of input used for
      processing 3D models by neural networks - voxels, point clouds and 2D
      images taken from multiple views. Although the conversion is implemented
      in multi-threaded fashion it still can be quite time consuming on big
      datasets, it can take up to ten or more hours in some cases.</p>
    <h2>Voxels</h2>
    <p><b>Docker</b><b>: </b><i>dockers/data_conversion/vrnens_data</i><br>
      <b>Networks: </b><i>dockers/vrnens</i><b><br>
      </b></p>
    <p><b>Details: </b>Voxels are natural extension of pixels to three
      dimensions. Instead of square lattice we have cube occupancy grid. For
      voxelization of meshes we use <a href="https://www.openvdb.org/" target="_blank">openvdb
        library</a>, which is written in c++ but offers some basic functionality
      including voxelization in python. Only one of our networks uses directly
      voxels as their input and for the purposes of this network data is stored
      in <i>.npz </i>format. Parameters include <code>num_voxels </code>which
      is resolution of the voxel grid and <code>num_rotations</code> which is
      number of rotations of single 3D model to be voxelized. </p>
    <h2>Images</h2>
    <p>Multi view neural networks get multiple rendered images of a single 3D
      model as their input. This rendering is computationally most demanding of
      all types of data conversion. Fortunately all the multi view networks
      accept the same format.</p>
    <p><b>Docker</b><b>: </b><i>dockers/data_conversion/pbrt_data</i><br>
      <b>Networks: </b><i>dockers/mvcnn, </i><i>dockers/mvcnn2, </i><i>dockers/vgg,
        </i><i>dockers/rotnet</i></p>
    <p>This is our implementation of rendering using <a href="https://www.pbrt.org/">pbrt</a>.
      It can be very slow, although we are sure that the implementation could be
      improved. You can set the number of rendered views as parameter <code>num_views</code>.
      If the <code>dodecahedron</code> parameter is set to True, twenty views
      from the vertices of regular dodecahedron will be rendered. This is useful
      when working with dataset without canonical rotation. You can also use
      more rotations of the camera from a single viewpoint by setting <code>camera_rotations</code>
      parameter. Pbrt can use only <i>.obj</i> files so for ModelNet .<i>off</i>
      files are converted to this format and saved. Set <i>remove_obj = False </i>if
      you want to keep these files after rendering is done.</p>
    <p><b>Docker</b><b>: </b><i>dockers/data_conversion/blender_data</i><br>
      <b>Networks: </b><i>dockers/mvcnn, </i><i>dockers/mvcnn2, </i><i>dockers/vgg,
        </i><i>dockers/rotnet</i></p>
    <p>With better success we used scripts for blender provided by team around <a
        href="https://people.cs.umass.edu/%7Ejcsu/papers/shape_recog/">this
        paper</a>. We just connected these into our framework, the parameters
      are same as above. There are two options shaded images and depth images.
      For details consult the original paper. You can set the mode of rendering
      by setting variable <code>render </code>in <i>run.sh</i>.</p>
    <h2></h2>
    <h2>Point clouds</h2>
    <p>Point cloud is simply set of points in three dimensional space. It can be
      obtained fairly easily from meshes by sampling random points from the
      faces. The probability of the face being selected is weighted by its area.
    </p>
    <p><b>Docker</b><b>: </b><i>dockers/data_conversion/pnet_data</i><br>
      <b>Networks: </b><i>dockers/pointnet,</i><b> </b><i>dockers/pointnet2</i></p>
    <p>For pointnet and its successor pointnet++ we save the sampled point data
      in several <i>.h5 </i>files which are listed in text files. With all
      parameters unchanged, all paths will be valid, but make sure that the
      paths in these text files are correct inside the docker container. You can
      set up number of sampled points as a parameter. You can also set <code>normal
        = True </code>to sample the points with surface normals, but the
      pointnet implementation of this is currently not working.</p>
    <p><b>Docker</b><b>: </b><i>dockers/data_conversion/sonet_data</i><br>
      <b>Networks: </b><i>dockers/sonet</i></p>
    <p>Sonet data is sampled in the same way as pointnet with surface normals.
      Then it creates and learns self organizing map to better represent the
      data. This map is created during the data conversion and therefore this
      script requires GPU and nvidia runtime. You can set the number of som
      nodes as parameter <code>num_nodes </code>and this should be a square
      integer.</p>
    <h2><code><span style="font-family: serif;"><span style="font-family: monospace;"></span></span></code><span
        style="font-family: serif;"><span style="font-family: monospace;"></span></span><span
        style="font-family: serif;"><span style="font-family: monospace;"></span></span><span
        style="font-family: serif;"><span style="font-family: monospace;"></span></span>Other</h2>
    <p> </p>
    <p><b>Docker</b><b>: </b><i>dockers/data_conversion/octree_data</i><br>
      <b>Networks: </b><i>dockers/pointnet,</i><b> </b><i>dockers/octree, </i><i>dockers/pointnet,</i><b>
      </b><i>dockers/octree_adaptive</i></p>
    <p>Octree is a data structure for storing 3D data efficiently. This script
      which uses tools provided by the authors of original paper. Data is
      augmented by creating more than one rotation of the 3D model and you can
      control this number by setting <code>num_rotations</code> parameter. If
      you want to create data for adaptive octree network set <code>adaptive =
        True</code>. There is also an option to correct some badly written <i>.off
        </i>files in ModelNet40.</p>
    <p><b>Docker</b><b>: </b><i>dockers/data_conversion/kdnet_data</i><br>
      <b>Networks: </b><i>dockers/pointnet,</i><b> </b><i>dockers/kdnet</i></p>
    <p>This script only loads the data and saves them in a large <i>.h5</i>
      file. Construction of kd-trees which are input of the kd-network is
      contructed during training.</p>
    <p> </p>
    <h2><a href="Manual.html"><b>&lt;&lt;&lt; BACK</b></a></h2>
  </body>
</html>
